{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import dataclasses\n",
    "from llama import ModelArgs\n",
    "from llama import Transformer, TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"dim\": 4096,            # hidden dim 隐藏层维度\n",
    "    \"n_layers\": 32,         # transformer block层数\n",
    "    \"n_heads\": 32,          # 自注意力head数量\n",
    "    \"vocab_size\": 32000,    # 词表大小\n",
    "    \"multiple_of\": 256,     # 用于计算transformer block 前向传播隐藏层维度\n",
    "    \"norm_eps\": 1e-06,\n",
    "}\n",
    "model_args = ModelArgs(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init model ...\n"
     ]
    }
   ],
   "source": [
    "print(\"init model ...\")\n",
    "llama_model = Transformer(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x TransformerBlock(\n",
      "      (attention): Attention(\n",
      "        (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llama_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型参数量\n",
    "def stats_module_params(module: nn.Module):\n",
    "    return sum([p.numel() for p in module.parameters()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA模型参数量计算公式：\n",
    "$$vocabSize * dim + nLayers * (dim * dim * 4 + dim * ffDim * 3 + dim + dim) + (dim + dim * vocabSize) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_params(args: ModelArgs):\n",
    "    # 计算transformer block中前向传播隐藏层的维度\n",
    "    ff_hidden_dim = int(2 * args.dim * 4 / 3)\n",
    "    ff_hidden_dim = args.multiple_of * ((ff_hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
    "\n",
    "    # 根据模型配置统计模型参数量\n",
    "    # 1. Embedding层\n",
    "    # 2. Transformer block层\n",
    "    # 3. 输出层\n",
    "    total_params = args.vocab_size * args.dim \\\n",
    "                   + args.n_layers * (args.dim * args.dim * 4 + args.dim * ff_hidden_dim * 3 + args.dim + args.dim) \\\n",
    "                   + args.dim + args.dim * args.vocab_size\n",
    "                   \n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama 7B model params: 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "assert stats_module_params(llama_model) == llama_params(model_args)\n",
    "print(f\"llama 7B model params: {llama_params(model_args):,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed params:\t 131,072,000\n",
      "layers params:\t 6,476,267,520\n",
      "norm params:\t 4,096\n",
      "output params:\t 131,072,000\n",
      "total params:\t 6,738,415,616\n"
     ]
    }
   ],
   "source": [
    "print(f\"embed params:\\t {stats_module_params(llama_model.tok_embeddings):,d}\")\n",
    "print(f\"layers params:\\t {stats_module_params(llama_model.layers):,d}\")\n",
    "print(f\"norm params:\\t {stats_module_params(llama_model.norm):,d}\")\n",
    "print(f\"output params:\\t {stats_module_params(llama_model.output):,d}\")\n",
    "print(f\"total params:\\t {stats_module_params(llama_model):,d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55a26945acae1dbe174fa8a7f2737f59bcc9ca988f8fc990f33e458e609cda8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
