{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function langchain.embeddings.llamacpp.LlamaCppEmbeddings.embed_documents(self, texts: List[str]) -> List[List[float]]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import pickle\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "LlamaCppEmbeddings.embed_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize chain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd/miniconda3/envs/torch/lib/python3.8/site-packages/langchain/chains/qa_with_sources/vector_db.py:60: UserWarning: `VectorDBQAWithSourcesChain` is deprecated - please use `from langchain.chains import RetrievalQAWithSourcesChain`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"faiss_store.pkl\", \"rb\") as f:\n",
    "    store = pickle.load(f)\n",
    "\n",
    "store.index = faiss.read_index(\"docs.index\")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"使用上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答案。\n",
    "\n",
    "{context}\n",
    "\n",
    "问题: {question}\n",
    "中文答案:\"\"\"\n",
    "\n",
    "print(\"initialize chain\")\n",
    "chain = VectorDBQAWithSourcesChain.from_llm(llm=OpenAI(temperature=0),\n",
    "                                            vectorstore=store,\n",
    "                                            question_prompt=PromptTemplate(template=prompt_template, \n",
    "                                                                           input_variables=[\"context\", \"question\"]))\n",
    "\n",
    "\n",
    "def search(query: str):\n",
    "    # query = \"Transformer的网络结构\"\n",
    "    print(f\"问题: {query}\")\n",
    "    result = chain({\"question\": query}, return_only_outputs=True)\n",
    "    # print(json.dumps(result, ensure_ascii=False, indent=4))\n",
    "    print(f\"答案: {result['answer']}\")\n",
    "    print(f\"来源: {result['sources']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题: Wenet模型网络结构\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2297 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答案:  Wenet model network structure is a hybrid Connectionist Time Classification (CTC)/Attention architecture, where Transformers or Conformers are used as encoders and Attention decoders are used to re-score the CTC hypotheses. To achieve a unified model for streaming and non-streaming, we use a dynamic block-based attention policy that allows self-attention to focus on the correct context with random.\n",
      "\n",
      "来源: /data/datasets/papers/Wenet-Production-Oriented-Streaming-and-Non-streaming-End-to-End-Speech-Recognition-Toolkit.pdf\n",
      "\n",
      "\n",
      "问题: Wenet解码方式有哪些？\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2070 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答案:  Wenet supports n-gram, WFST, CTC prefix beam search, CTC WFST search, and attention re-scoring decoding methods.\n",
      "\n",
      "来源: /data/datasets/papers/Wenet-Production-Oriented-Streaming-and-Non-streaming-End-to-End-Speech-Recognition-Toolkit.pdf, /data/datasets/papers/Wenet2.0-More-Productive-End-to-End-Speech-Recognition-Toolkit.pdf\n",
      "\n",
      "\n",
      "问题: 介绍下megatron框架\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2576 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答案:  Megatron is a large-scale transformer language model training framework developed jointly by Microsoft and NVIDIA. It can use DeepSpeed and Megatron to train super large-scale transformer language models and can adapt to downstream tasks through zero-shot, few-shot, and fine-tuning techniques. It is a distributed multi-processing framework based on NVIDIA that can help users train large models more efficiently. It supports various distributed training techniques, including data parallelism (DP), model parallelism (MP), and parameter parallelism (PP). Megatron-LM is a model parallelism technique based on Megatron that can help users train large models more efficiently. It combines tensor parallelism, pipeline parallelism, and data parallelism to scale up to thousands of GPUs. It also proposes a new interleaved pipeline scheduling that can improve throughput by more than 10% with memory usage comparable to existing methods.\n",
      "\n",
      "来源: /data/datasets/papers/MP-Using-DeepSpeed-and-Megatron-to-Train-Megatron-Turing-NLG-530B-A-Large-Scale-Generative-Language-Model.pdf\n",
      "/data/datasets/papers/MP-ZeRO-Memory-Opt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query_list = [\n",
    "    # \"详细介绍下Transformer architecture\",\n",
    "    # \"什么是pipeline parallel?\",\n",
    "    # \"详细介绍下Data parallel\",\n",
    "    \"Wenet模型网络结构\",\n",
    "    \"Wenet解码方式有哪些？\",\n",
    "    \"介绍下megatron框架\",\n",
    "]\n",
    "\n",
    "for query in query_list:\n",
    "    search(query)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55a26945acae1dbe174fa8a7f2737f59bcc9ca988f8fc990f33e458e609cda8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
